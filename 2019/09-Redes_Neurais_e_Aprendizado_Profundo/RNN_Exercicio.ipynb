{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alterando a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desta prática é que você possa alterar a LSTM apresentada durante a aula e avaliar o resultado da rede alterada. Algumas possibilidades são listadas abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionar mais camadas LSTM\n",
    "\n",
    "Você ainda pode adicionar mais camadas LSTM (uma pilha de LSTMs!), também chamada de \"stacked LSTM\". Para isso, uma LSTM precisa retornar uma sequência de valores para a LSTM da próxima camada. Isto é feito passando o parâmetro return_sequences=True na criação da camada LSTM. Mas lembre-se, a última camada não deve retornar a sequência e sim um único valor como fizemos até agora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(new_dim, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Como saber o número adequado de épocas para o treino? Usando poucas épocas podemos não aprender o suficiente. Ao contrário, usando muitas épocas podemos ter um overfitting (quando modelo se adequa muito aos dados de treino). Early Stopping (ou parada antecipada) pode ser utilizado para resolver este problema. No keras esta técnica é implementada como um callback que é passado no parâmetro **callbacks** (que é uma lista de callbacks) para o método fit. A ideia é durante o treino verificar se determinada medida (parâmetro monitor) não melhorou de maneira suficiente durante um determinado número de épocas (parâmetro patience).\n",
    "\n",
    "https://keras.io/callbacks/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor=\"var_acc\", patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada Droptout\n",
    "\n",
    "Uma forma de reduzir o overfitting de um modelo de redes neurais é utilizar uma camada de **dropout**. A idéia-chave é descartar aleatoriamente unidades (juntamente com suas conexões) da rede  neural durante o treinamento. Isso evita que as unidades adaptem demais aos dados de treino. No Keras, o dropout é implementado pelo módulo keras.layers.Dropout.\n",
    "https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning do batch size\n",
    "O batch size controla a frequência em que os pesos da rede serão alterados. Compare o efeito de diferentes batch sizes para o modelo modificado. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in [16, 32,64,128]:\n",
    "     ## treine e avalie o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning de outros parâmetros\n",
    "Você também pode avaliar outros parâmetros, como por exemplo, o tamanho dos vetores no espaço gerado pela camada embedding, o tamanho máximo das sequencias de entrada, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
